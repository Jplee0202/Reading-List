#  Reading-List

## Reasoning
* **Neural Module Networks:** Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, 2015[[Blog](https://bair.berkeley.edu/blog/2017/06/20/learning-to-reason-with-neural-module-networks/)][[arxiv](https://arxiv.org/abs/1511.02799)]
    * Instead of a huge network, but multiple nerual moudles which implement different steps of the reasoning.
 ## Pre-trained Model
* **Attention Is All You Need:** Ashish Vaswani, Noam Shazeer, 2017
    * New encoder + decoder architecture with self-attention mechanism for machine translation
* **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**  Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018
    * Unsupervised pre-training (mask+next sentence prediction) to get contextual representation
    * Fine-tuing for downstream tasks

* **ALBERT: A Lite BERT for Self-supervised Learning of Language Representations**, Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut,2019
    * Coming soon...
* **VisualBERT: A Simple and Performant Baseline for Vision and Language**,  Liunian Harold Li, Mark Yatskar,  Da Yin, Cho-Jui Hsieh & Kai-Wei Chang, 2019
    * Coming soon... 


## Medical report generation
* **On the Automatic Generation of Medical Imaging Reports:** Baoyu Jing, Pengtao Xie, Eric Xing, 2017

  Basic pipeline: Adopting attention mechanism to align visual feature and language feature. Hierachical LSTM for text generation
  [[arxiv](https://arxiv.org/abs/1711.08195)]
